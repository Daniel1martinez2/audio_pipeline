{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "933426a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('/Users/danielmartinezvillegas/Developer/master-ds/✨TDG/audio_pipeline/data/dataset_jose_fwod_vector.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7254d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorToVectorDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        x = torch.tensor([row[f'x_{i}'] for i in range(16)], dtype=torch.float32)\n",
    "        y = torch.tensor([row[f'y_{i}'] for i in range(16)], dtype=torch.float32)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "219c39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleMLP(nn.Module):\n",
    "    def __init__(self, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(16, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1e5572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=30):\n",
    "    model.to(device)\n",
    "    best_model = model.state_dict()\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_hat = model(x)\n",
    "                val_loss += criterion(y_hat, y).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2056e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(df, device, n_models=5):\n",
    "    dataset = VectorToVectorDataset(df)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=64)\n",
    "\n",
    "    ensemble = []\n",
    "    for i in range(n_models):\n",
    "        print(f\"\\nTraining model {i+1}/{n_models}\")\n",
    "        model = EnsembleMLP()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        criterion = nn.MSELoss()\n",
    "        trained_model = train_single_model(model, train_loader, val_loader, criterion, optimizer, device)\n",
    "        ensemble.append(trained_model)\n",
    "\n",
    "    return ensemble, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "383265f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, x):\n",
    "    return torch.stack([m(x) for m in models]).mean(0)\n",
    "\n",
    "def evaluate_ensemble(models, val_loader, device):\n",
    "    all_preds, all_targets = [], []\n",
    "    for x, y in val_loader:\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = ensemble_predict(models, x).cpu().numpy()\n",
    "        all_preds.append(pred)\n",
    "        all_targets.append(y.numpy())\n",
    "\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_targets = np.vstack(all_targets)\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    print(f\"\\n✅ Ensemble MSE: {mse:.4f}\")\n",
    "\n",
    "    for i in range(min(3, len(all_preds))):\n",
    "        plt.figure(figsize=(10, 2))\n",
    "        plt.plot(all_targets[i], label='True', marker='o')\n",
    "        plt.plot(all_preds[i], label='Predicted', marker='x')\n",
    "        plt.title(f'Example {i+1}')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "440f2f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model 1/5\n",
      "Epoch 1/30 - Train Loss: 0.1035, Val Loss: 0.0922\n",
      "Epoch 2/30 - Train Loss: 0.0936, Val Loss: 0.0901\n",
      "Epoch 3/30 - Train Loss: 0.0918, Val Loss: 0.0881\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Entrenar el ensemble\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m models, val_loader \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Evaluar el rendimiento del ensemble\u001b[39;00m\n\u001b[1;32m      7\u001b[0m evaluate_ensemble(models, val_loader, device)\n",
      "Cell \u001b[0;32mIn[52], line 16\u001b[0m, in \u001b[0;36mtrain_ensemble\u001b[0;34m(df, device, n_models)\u001b[0m\n\u001b[1;32m     14\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m     15\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m---> 16\u001b[0m     trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_single_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     ensemble\u001b[38;5;241m.\u001b[39mappend(trained_model)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ensemble, val_loader\n",
      "Cell \u001b[0;32mIn[51], line 14\u001b[0m, in \u001b[0;36mtrain_single_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_hat, y)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/personal/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/personal/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/personal/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Entrenar el ensemble\n",
    "models, val_loader = train_ensemble(df, device)\n",
    "\n",
    "# Evaluar el rendimiento del ensemble\n",
    "evaluate_ensemble(models, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ccbcf3",
   "metadata": {},
   "source": [
    "# Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b985d0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Training model 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 370/370 [00:01<00:00, 204.19it/s, train_loss=0.0915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 0.1159, Val Loss: 0.0948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 370/370 [00:01<00:00, 206.03it/s, train_loss=0.1111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 - Train Loss: 0.0962, Val Loss: 0.0929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 370/370 [00:01<00:00, 198.76it/s, train_loss=0.0982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 - Train Loss: 0.0938, Val Loss: 0.0904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 370/370 [00:01<00:00, 203.02it/s, train_loss=0.0833]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 - Train Loss: 0.0926, Val Loss: 0.0884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 370/370 [00:01<00:00, 187.11it/s, train_loss=0.0892]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 - Train Loss: 0.0913, Val Loss: 0.0880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 370/370 [00:01<00:00, 196.30it/s, train_loss=0.0730]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 - Train Loss: 0.0907, Val Loss: 0.0872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 370/370 [00:01<00:00, 197.72it/s, train_loss=0.0935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 - Train Loss: 0.0894, Val Loss: 0.0866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 370/370 [00:02<00:00, 177.53it/s, train_loss=0.1094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 - Train Loss: 0.0889, Val Loss: 0.0851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 370/370 [00:01<00:00, 202.47it/s, train_loss=0.0887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 - Train Loss: 0.0883, Val Loss: 0.0848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 370/370 [00:01<00:00, 196.32it/s, train_loss=0.0971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 - Train Loss: 0.0879, Val Loss: 0.0844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 370/370 [00:01<00:00, 204.44it/s, train_loss=0.0980]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 - Train Loss: 0.0873, Val Loss: 0.0852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 370/370 [00:02<00:00, 171.15it/s, train_loss=0.0747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 - Train Loss: 0.0866, Val Loss: 0.0832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 370/370 [00:01<00:00, 195.88it/s, train_loss=0.0808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 - Train Loss: 0.0863, Val Loss: 0.0834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|██████████| 370/370 [00:01<00:00, 201.95it/s, train_loss=0.0909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 - Train Loss: 0.0861, Val Loss: 0.0832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|██████████| 370/370 [00:01<00:00, 197.25it/s, train_loss=0.0793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 - Train Loss: 0.0856, Val Loss: 0.0826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|██████████| 370/370 [00:01<00:00, 207.63it/s, train_loss=0.0889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 - Train Loss: 0.0852, Val Loss: 0.0822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|██████████| 370/370 [00:02<00:00, 180.45it/s, train_loss=0.0823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 - Train Loss: 0.0853, Val Loss: 0.0821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|██████████| 370/370 [00:01<00:00, 200.38it/s, train_loss=0.0768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 - Train Loss: 0.0847, Val Loss: 0.0820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|██████████| 370/370 [00:01<00:00, 192.56it/s, train_loss=0.0974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 - Train Loss: 0.0846, Val Loss: 0.0819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|██████████| 370/370 [00:01<00:00, 202.18it/s, train_loss=0.0972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 - Train Loss: 0.0846, Val Loss: 0.0818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|██████████| 370/370 [00:01<00:00, 207.24it/s, train_loss=0.0912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 - Train Loss: 0.0841, Val Loss: 0.0806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|██████████| 370/370 [00:01<00:00, 188.28it/s, train_loss=0.1043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 - Train Loss: 0.0840, Val Loss: 0.0807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|██████████| 370/370 [00:01<00:00, 193.92it/s, train_loss=0.1066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 - Train Loss: 0.0840, Val Loss: 0.0803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|██████████| 370/370 [00:01<00:00, 196.08it/s, train_loss=0.0921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 - Train Loss: 0.0838, Val Loss: 0.0811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|██████████| 370/370 [00:01<00:00, 208.83it/s, train_loss=0.0964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 - Train Loss: 0.0836, Val Loss: 0.0800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|██████████| 370/370 [00:01<00:00, 207.55it/s, train_loss=0.0863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 - Train Loss: 0.0833, Val Loss: 0.0802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|██████████| 370/370 [00:01<00:00, 194.13it/s, train_loss=0.0866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100 - Train Loss: 0.0831, Val Loss: 0.0800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: 100%|██████████| 370/370 [00:01<00:00, 209.93it/s, train_loss=0.0948]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 - Train Loss: 0.0832, Val Loss: 0.0795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: 100%|██████████| 370/370 [00:01<00:00, 199.61it/s, train_loss=0.0780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100 - Train Loss: 0.0832, Val Loss: 0.0808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: 100%|██████████| 370/370 [00:01<00:00, 210.65it/s, train_loss=0.0885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 - Train Loss: 0.0828, Val Loss: 0.0796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: 100%|██████████| 370/370 [00:01<00:00, 204.00it/s, train_loss=0.0818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100 - Train Loss: 0.0828, Val Loss: 0.0807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: 100%|██████████| 370/370 [00:01<00:00, 188.60it/s, train_loss=0.0947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100 - Train Loss: 0.0828, Val Loss: 0.0801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100: 100%|██████████| 370/370 [00:01<00:00, 204.54it/s, train_loss=0.0899]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100 - Train Loss: 0.0827, Val Loss: 0.0810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100: 100%|██████████| 370/370 [00:01<00:00, 190.25it/s, train_loss=0.0784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 - Train Loss: 0.0824, Val Loss: 0.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100: 100%|██████████| 370/370 [00:01<00:00, 194.87it/s, train_loss=0.0804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100 - Train Loss: 0.0823, Val Loss: 0.0794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100: 100%|██████████| 370/370 [00:01<00:00, 210.19it/s, train_loss=0.0974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100 - Train Loss: 0.0824, Val Loss: 0.0794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100: 100%|██████████| 370/370 [00:02<00:00, 146.59it/s, train_loss=0.1039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100 - Train Loss: 0.0822, Val Loss: 0.0794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100: 100%|██████████| 370/370 [00:01<00:00, 193.75it/s, train_loss=0.0811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100 - Train Loss: 0.0823, Val Loss: 0.0788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/100: 100%|██████████| 370/370 [00:01<00:00, 208.72it/s, train_loss=0.0876]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100 - Train Loss: 0.0821, Val Loss: 0.0793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100: 100%|██████████| 370/370 [00:01<00:00, 187.94it/s, train_loss=0.0702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100 - Train Loss: 0.0821, Val Loss: 0.0786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100: 100%|██████████| 370/370 [00:01<00:00, 203.57it/s, train_loss=0.0781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100 - Train Loss: 0.0817, Val Loss: 0.0793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100: 100%|██████████| 370/370 [00:01<00:00, 216.91it/s, train_loss=0.1006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100 - Train Loss: 0.0819, Val Loss: 0.0787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100: 100%|██████████| 370/370 [00:01<00:00, 209.95it/s, train_loss=0.0942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100 - Train Loss: 0.0817, Val Loss: 0.0779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/100: 100%|██████████| 370/370 [00:01<00:00, 195.17it/s, train_loss=0.1016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100 - Train Loss: 0.0818, Val Loss: 0.0784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/100: 100%|██████████| 370/370 [00:01<00:00, 213.43it/s, train_loss=0.0667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100 - Train Loss: 0.0816, Val Loss: 0.0784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/100: 100%|██████████| 370/370 [00:01<00:00, 193.30it/s, train_loss=0.0767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100 - Train Loss: 0.0816, Val Loss: 0.0782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/100: 100%|██████████| 370/370 [00:01<00:00, 193.49it/s, train_loss=0.0932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100 - Train Loss: 0.0817, Val Loss: 0.0785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/100: 100%|██████████| 370/370 [00:01<00:00, 197.78it/s, train_loss=0.0947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100 - Train Loss: 0.0818, Val Loss: 0.0779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/100: 100%|██████████| 370/370 [00:01<00:00, 201.54it/s, train_loss=0.0910]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100 - Train Loss: 0.0816, Val Loss: 0.0785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/100: 100%|██████████| 370/370 [00:01<00:00, 202.88it/s, train_loss=0.0973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100 - Train Loss: 0.0813, Val Loss: 0.0781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/100: 100%|██████████| 370/370 [00:01<00:00, 202.58it/s, train_loss=0.0832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100 - Train Loss: 0.0813, Val Loss: 0.0779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/100: 100%|██████████| 370/370 [00:01<00:00, 206.52it/s, train_loss=0.0879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100 - Train Loss: 0.0813, Val Loss: 0.0788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/100: 100%|██████████| 370/370 [00:02<00:00, 184.82it/s, train_loss=0.0799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100 - Train Loss: 0.0812, Val Loss: 0.0782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/100: 100%|██████████| 370/370 [00:01<00:00, 195.81it/s, train_loss=0.0809]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100 - Train Loss: 0.0813, Val Loss: 0.0788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/100: 100%|██████████| 370/370 [00:01<00:00, 191.11it/s, train_loss=0.0748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100 - Train Loss: 0.0805, Val Loss: 0.0778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/100: 100%|██████████| 370/370 [00:01<00:00, 197.88it/s, train_loss=0.1045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100 - Train Loss: 0.0803, Val Loss: 0.0773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/100: 100%|██████████| 370/370 [00:01<00:00, 198.70it/s, train_loss=0.0768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100 - Train Loss: 0.0802, Val Loss: 0.0769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/100: 100%|██████████| 370/370 [00:01<00:00, 205.97it/s, train_loss=0.0713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100 - Train Loss: 0.0802, Val Loss: 0.0770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/100: 100%|██████████| 370/370 [00:01<00:00, 198.72it/s, train_loss=0.0785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100 - Train Loss: 0.0802, Val Loss: 0.0773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/100: 100%|██████████| 370/370 [00:01<00:00, 202.53it/s, train_loss=0.0803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100 - Train Loss: 0.0801, Val Loss: 0.0776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/100:  63%|██████▎   | 233/370 [00:01<00:00, 185.34it/s, train_loss=0.0939]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 355\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# Train the ensemble\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m models, model_weights, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# Evaluate the performance of the ensemble\u001b[39;00m\n\u001b[1;32m    358\u001b[0m evaluate_ensemble(models, model_weights, test_loader, device)\n",
      "Cell \u001b[0;32mIn[11], line 268\u001b[0m, in \u001b[0;36mtrain_ensemble\u001b[0;34m(df, device, n_models)\u001b[0m\n\u001b[1;32m    265\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m combined_loss\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m trained_model, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_single_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increased epochs with early stopping\u001b[39;49;00m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\n\u001b[1;32m    278\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# Calculate model weight based on validation performance\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Lower validation loss = higher weight\u001b[39;00m\n\u001b[1;32m    282\u001b[0m weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (val_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 165\u001b[0m, in \u001b[0;36mtrain_single_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, scheduler, epochs, patience)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m--> 165\u001b[0m         x, y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    166\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    167\u001b[0m         y_hat \u001b[38;5;241m=\u001b[39m model(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/Users/danielmartinezvillegas/Developer/master-ds/✨TDG/audio_pipeline/data/dataset_jose_fwod_vector.csv')\n",
    "\n",
    "# Normalize input features to improve training stability\n",
    "for i in range(16):\n",
    "    col = f'x_{i}'\n",
    "    mean, std = df[col].mean(), df[col].std()\n",
    "    if std > 0:  # Avoid division by zero\n",
    "        df[col] = (df[col] - mean) / std\n",
    "\n",
    "class VectorToVectorDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        x = torch.tensor([row[f'x_{i}'] for i in range(16)], dtype=torch.float32)\n",
    "        y = torch.tensor([row[f'y_{i}'] for i in range(16)], dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "# Define different model architectures for ensemble diversity\n",
    "class ImprovedMLP(nn.Module):\n",
    "    def __init__(self, dropout=0.2, use_residual=True, activation='leaky_relu'):\n",
    "        super().__init__()\n",
    "        self.use_residual = use_residual\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'leaky_relu':\n",
    "            self.act = nn.LeakyReLU(0.1)\n",
    "        elif activation == 'gelu':\n",
    "            self.act = nn.GELU()\n",
    "        else:\n",
    "            self.act = nn.ReLU()\n",
    "            \n",
    "        # Layer blocks\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(16, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            self.act,\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            self.act,\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            self.act\n",
    "        )\n",
    "        \n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            self.act\n",
    "        )\n",
    "        \n",
    "        # Output layer - linear output for flexibility\n",
    "        self.output = nn.Linear(32, 16)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b1 = self.block1(x)\n",
    "        b2 = self.block2(b1)\n",
    "        \n",
    "        # Residual connection if dimensions match\n",
    "        if self.use_residual:\n",
    "            b2 = b2 + b1\n",
    "            \n",
    "        b3 = self.block3(b2)\n",
    "        b4 = self.block4(b3)\n",
    "        out = self.output(b4)\n",
    "        return out\n",
    "\n",
    "# Wider network architecture for ensemble diversity\n",
    "class WiderMLP(nn.Module):\n",
    "    def __init__(self, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(16, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, 16)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Deeper network architecture for ensemble diversity\n",
    "class DeeperMLP(nn.Module):\n",
    "    def __init__(self, dropout=0.25):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(16, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def combined_loss(pred, target, alpha=0.8):\n",
    "    # Combine MSE and Huber loss for robustness\n",
    "    mse_loss = F.mse_loss(pred, target)\n",
    "    huber_loss = F.smooth_l1_loss(pred, target)\n",
    "    return alpha * mse_loss + (1 - alpha) * huber_loss\n",
    "\n",
    "def train_single_model(model, train_loader, val_loader, criterion, optimizer, device, scheduler=None, epochs=50, patience=10):\n",
    "    model.to(device)\n",
    "    best_model = model.state_dict()\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve_count = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # Use tqdm for progress bar\n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n",
    "            for x, y in pbar:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                y_hat = model(x)\n",
    "                loss = criterion(y_hat, y)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping to prevent exploding gradients\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                pbar.set_postfix({\"train_loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_hat = model(x)\n",
    "                val_loss += criterion(y_hat, y).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Step the learning rate scheduler if provided\n",
    "        if scheduler is not None:\n",
    "            # For ReduceLROnPlateau, we need to pass the validation loss\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            no_improve_count = 0\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "            \n",
    "        # Early stopping\n",
    "        if no_improve_count >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    return model, best_val_loss\n",
    "\n",
    "def create_model_architecture(model_type, dropout=0.3):\n",
    "    if model_type == 'improved':\n",
    "        return ImprovedMLP(dropout=dropout, use_residual=True, activation='leaky_relu')\n",
    "    elif model_type == 'wider':\n",
    "        return WiderMLP(dropout=dropout)\n",
    "    elif model_type == 'deeper':\n",
    "        return DeeperMLP(dropout=dropout)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "def train_ensemble(df, device, n_models=7):\n",
    "    # Split data\n",
    "    dataset = VectorToVectorDataset(df)\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    val_size = int(0.15 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    \n",
    "    train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=64)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64)\n",
    "\n",
    "    ensemble = []\n",
    "    model_weights = []\n",
    "    \n",
    "    # Different model architectures to try\n",
    "    model_types = ['improved', 'wider', 'deeper']\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        print(f\"\\nTraining model {i+1}/{n_models}\")\n",
    "        \n",
    "        # Vary model architecture, dropout rate\n",
    "        model_type = model_types[i % len(model_types)]\n",
    "        dropout = np.random.uniform(0.1, 0.4)\n",
    "        \n",
    "        model = create_model_architecture(model_type, dropout)\n",
    "        \n",
    "        # Try different optimizers\n",
    "        if i % 3 == 0:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "        elif i % 3 == 1:\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "        else:\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "            \n",
    "        # Learning rate scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "        \n",
    "        # Criterion\n",
    "        if i % 2 == 0:\n",
    "            criterion = nn.MSELoss()\n",
    "        else:\n",
    "            criterion = combined_loss\n",
    "            \n",
    "        # Train the model\n",
    "        trained_model, val_loss = train_single_model(\n",
    "            model, \n",
    "            train_loader, \n",
    "            val_loader, \n",
    "            criterion, \n",
    "            optimizer, \n",
    "            device,\n",
    "            scheduler=scheduler,\n",
    "            epochs=100,  # Increased epochs with early stopping\n",
    "            patience=15\n",
    "        )\n",
    "        \n",
    "        # Calculate model weight based on validation performance\n",
    "        # Lower validation loss = higher weight\n",
    "        weight = 1.0 / (val_loss + 1e-5)\n",
    "        \n",
    "        ensemble.append(trained_model)\n",
    "        model_weights.append(weight)\n",
    "        \n",
    "    # Normalize weights\n",
    "    total_weight = sum(model_weights)\n",
    "    model_weights = [w / total_weight for w in model_weights]\n",
    "    \n",
    "    return ensemble, model_weights, test_loader\n",
    "\n",
    "def weighted_ensemble_predict(models, weights, x):\n",
    "    predictions = []\n",
    "    for i, model in enumerate(models):\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)\n",
    "        predictions.append(pred * weights[i])\n",
    "    return torch.stack(predictions).sum(0)\n",
    "\n",
    "def evaluate_ensemble(models, weights, test_loader, device):\n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    models = [model.to(device) for model in models]\n",
    "    \n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    \n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = weighted_ensemble_predict(models, weights, x).cpu().numpy()\n",
    "        all_preds.append(pred)\n",
    "        all_targets.append(y.numpy())\n",
    "\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_targets = np.vstack(all_targets)\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    print(f\"\\n✅ Ensemble MSE: {mse:.4f}\")\n",
    "\n",
    "    # Calculate per-element MSE to find where the model struggles\n",
    "    element_mse = np.mean((all_targets - all_preds)**2, axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(range(16), element_mse)\n",
    "    plt.title('MSE by Output Element')\n",
    "    plt.xlabel('Output Element Index')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot a few examples\n",
    "    for i in range(min(3, len(all_preds))):\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        plt.plot(all_targets[i], label='True', marker='o')\n",
    "        plt.plot(all_preds[i], label='Predicted', marker='x')\n",
    "        plt.title(f'Example {i+1}')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seeds(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    set_seeds()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Train the ensemble\n",
    "    models, model_weights, test_loader = train_ensemble(df, device, n_models=7)\n",
    "\n",
    "    # Evaluate the performance of the ensemble\n",
    "    evaluate_ensemble(models, model_weights, test_loader, device)\n",
    "    \n",
    "    # Save the best model\n",
    "    torch.save({\n",
    "        'models': [model.state_dict() for model in models],\n",
    "        'weights': model_weights\n",
    "    }, 'improved_ensemble_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
